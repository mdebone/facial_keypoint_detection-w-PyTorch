{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df2c9083",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-ffc36f4f8eef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnotebook\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b1e85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a06b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMG_DIR = 'data/training/'\n",
    "TEST_IMG_DIR = 'data/test/'\n",
    "\n",
    "TRAIN_CSV = 'data/training_frames_keypoints.csv'\n",
    "TEST_CSV = 'data/test_frames_keypoints.csv'\n",
    "\n",
    "DEVICE = 'cpu'\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 140\n",
    "\n",
    "LR = 0.001\n",
    "EPOCHS = 2\n",
    "MODEL_nAME = 'resnet18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b390f9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand Facial Keypoint dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce9073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(TRAIN_CSV)\n",
    "test_df = pd.read_csv(TEST_CSV)\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "\n",
    "img_name = train_df['Unnamed: 0'].iloc[idx]\n",
    "img_path = TRAIN_IMG-DIR + img_name\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "key = train_df.iloc[idx][1:].to_numpy().reshape(-1, 2)\n",
    "print(key)\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.scatter(key[:,0], key[:,1], s = 4, c = 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1d52f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(train_df, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78716d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Custom Facial Keypoint Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fba0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7fe57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialKeyDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, df, img_data_dir, augmentations = None):\n",
    "        self.df = df\n",
    "        self.img_data_dir = img_data_dir\n",
    "        self.augmentations = augmentations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        img_name = self.df['Unnamed: 0'].iloc[idx]\n",
    "        img = cv2.imread(self.img_data_dir + img_name)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        key = self.df.iloc[idx][1:].to_numpy().reshape(-1,2)\n",
    "        \n",
    "        if self.augmentations != None:\n",
    "            augmented_data = self.augmentations(image = img, keypoints = key)\n",
    "            img = torch.from_numpy(augmented_data['image']).float()\n",
    "            key = torch.tensor(augmented_data['keypoints']).float()\n",
    "            \n",
    "        return img.permute(2,0,1), key.view(-1) # (Height,Width,Channel) -> (reveresed CHannel,Height,Width)\n",
    "                                                # Output dimension is back (68,2) -> (136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3106b5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_augs = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(),\n",
    "    #A.HorizontalFlip(p=0.5)\n",
    "], keypoint_params=A.KeypointParams(format='xy', remove_invisible = False))\n",
    "\n",
    "valid_test_augs = A.Compose([\n",
    "    A.Resize(IMG_SIZE, IMG_SIZE),\n",
    "    A.Normalize(),\n",
    "    #A.HorizontalFlip(p=0.5)\n",
    "], keypoint_params=A.KeypointParams(format='xy', remove_invisible = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05ad3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = FacialKeyDataset(train_df, TRAIN_IMG_DIR, train_augs)\n",
    "validset = FacialKeyDataset(valid_df, TRAIN_IMG_DIR, valid_test_augs)\n",
    "testset = FacialKeyDataset(valid_df, TEST_IMG_DIR, valid_test_augs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ca7ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total no. of examples present in trainset : {}\".format(len(trainset)))\n",
    "print(\"Total no. of examples present in validset : {}\".format(len(validset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e184706b",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, key = trainset[13]\n",
    "helper.imshow_with_key(img, key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d33bae",
   "metadata": {},
   "source": [
    "# Load Dataset into Batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfd275",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size = BATCH_SIZE)\n",
    "validloader = torch.utils.data.DataLoader(validset, batch_size = BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c8113",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total no. batches in trainloader : {}\".format(len(trainloader)))\n",
    "print(\"Total no. batches in validloader : {}\".format(len(validloader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b869c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in trainloader:\n",
    "    image, key = data\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a6c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total no. batches in trainloader : {}\".format(len(trainloader)))\n",
    "print(\"Total no. batches in validloader : {}\".format(len(validloader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6473e954",
   "metadata": {},
   "source": [
    "# Create Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab625301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259e92d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialKeyModel(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, model_name = MODEL_NAME):\n",
    "        super(FacialKeyModel, self).__init__()\n",
    "        \n",
    "        print(\"Loading Backbone : {}\".format(model_name))\n",
    "        self.backbone = timm.create_model(model_name, pretrained = True, num_classes = 136)\n",
    "        \n",
    "    def forward(self, images, key = None):\n",
    "        \n",
    "        logits = self.backbone(images)\n",
    "        \n",
    "        if key != None:\n",
    "            return logits, torch.nn.MSELoss()(logits, key)\n",
    "        \n",
    "        return logits  # return only outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76bdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FacialKeyModel()\n",
    "model.to(DEVICE);\n",
    "\n",
    "model(torch.rand(16,3,140,140)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b293e94f",
   "metadata": {},
   "source": [
    "# Trainer and Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1e7ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, trainloader, optimizer):\n",
    "    \n",
    "    train_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for data in tqdm(trainloader):   # useful to see progression of the loop\n",
    "        \n",
    "        images, keys = data\n",
    "        images, keys = images.to(DEVICE), keys.to(DEVICE)\n",
    "        \n",
    "        output, loss = model(images, keys)\n",
    "        \n",
    "        optimizer.zer0_grad()\n",
    "        loss.backward() # dw, db\n",
    "        optimizer.step() #useful to update w = w - lr*dw, b = ...\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        \n",
    "    return train_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7054cf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(model, validloader):\n",
    "    \n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    model.eval() # need to make sure model is not using dropout layers\n",
    "    \n",
    "    with torch.no_grad():    \n",
    "        for data in tqdm(validloader):   # useful to see progression of the loop\n",
    "        \n",
    "            images, keys = data\n",
    "            images, keys = images.to(DEVICE), keys.to(DEVICE)\n",
    "        \n",
    "            output, loss = model(images, keys)\n",
    "            valid_loss += loss.item()\n",
    "        \n",
    "        return valid_loss / len(validloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb279201",
   "metadata": {},
   "source": [
    "# Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466f51f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr = LR)\n",
    "best_valid_loss = np.Inf\n",
    "\n",
    "for i in range(EPOCHS): # only two because one will take a lot of time\n",
    "    \n",
    "    avg_train_loss = train_fn(model, trainloader, optimizer)\n",
    "    avg_valid_loss = eval_fn(model, validloader)\n",
    "    \n",
    "    if avg_valid_loss < best_valid_loss:\n",
    "        torch.save(model.state_dict(), 'FacialKeyModel.pt')\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        \n",
    "    print(\"Epoch : {} Train_loss : {}\".format(i+1, avg_train_loss))\n",
    "    print(\"Epoch : {} Valid_loss : {}\".format(i+1, avg_valid_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3a8c68",
   "metadata": {},
   "source": [
    "# Visualizing Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e156f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 85\n",
    "image, key = testset[index]\n",
    "\n",
    "model.load_state_dict(torch.load('[Colab]FacialKeyModel.pt', map_location = DEVICE))\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    image = image.to(DEVICE)\n",
    "    out_key = model(image.unsqueeze(0)) # (tensor expects bs to be in shape of bedsize, channel, height, width) but the image is channel, height, width\n",
    "    helper.compare_keypoints(image, key, out_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2013e52c",
   "metadata": {},
   "source": [
    "# Optional Task "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c80519",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755bfa40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
